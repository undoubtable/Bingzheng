{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b092d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "import nltk\n",
    "\n",
    "text = \"Hello, world! This is a test sentence for tokenization.\"\n",
    "# 先确保所需的 punkt 资源已安装，避免在第一次调用时抛出 LookupError\n",
    "for resource in ('punkt', 'punkt_tab'):\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}')\n",
    "    except LookupError:\n",
    "        print(f'NLTK resource {resource} not found. Downloading...')\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "print(\"Word Tokenization:\")\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d009ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens = ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 现在进行分词，并捕获任何剩余的 LookupError 以便给出明确提示\n",
    "try:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    print('tokens =', tokens)\n",
    "except LookupError as e:\n",
    "    print('Failed to tokenize: missing NLTK resource. Please run nltk.download(\"punkt\") or nltk.download(\"punkt_tab\") manually.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff19bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\admin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.375 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens = ['在', '一个', '遥远', '的', '城市', '里', '，', '清晨', '的', '阳光', '穿过', '高楼', '之间', '的', '缝隙', '，', '洒', '在', '尚未', '醒来', '的', '街道', '上', '。']\n",
      "词数 = 24\n"
     ]
    }
   ],
   "source": [
    "# 中文分词示例（使用 jieba）\n",
    "import jieba\n",
    "test1 = \"在一个遥远的城市里，清晨的阳光穿过高楼之间的缝隙，洒在尚未醒来的街道上。\"\n",
    "# 假设 test1 已在 notebook 中定义\n",
    "# test1 = \"在一个遥远的城市里，清晨的阳光穿过高楼之间的缝隙，洒在尚未醒来的街道上。\"\n",
    "# 精确模式，返回 list\n",
    "tokens = jieba.lcut(test1)\n",
    "print('tokens =', tokens)\n",
    "print('词数 =', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fef1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nngnr', ' ', 'yuhkvf', ' ', 'cf', ' ', 'g', ' ', 'vawadrqj', ' ', 'm', ' ', 'zyy', ' ', 'yicyvdh', ' ', 'lhxixe', ' ', 'ozb', ' ', 'qgpnqy', ' ', 'mp', ' ', 'srpchbnj', ' ', 'n', ' ', 'fkkel', ' ', 'g', ' ', 'rc', ' ', 'pedor', ' ', 'glxrvow', ' ', 'xeewv', ' ', 'laxte', ' ', 'zoftod', ' ', 'vlwqd', ' ', 'f', ' ', 'bcytlx', ' ', 'j', ' ', 'bkhvcel', ' ', 'ocrniemk', ' ', 'hz', ' ', 'rioucuj', ' ', 'pala', ' ', 'ixirgm气示符中气理好你智然随例好你字天你例试本界世好气智好界']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import nltk\n",
    "\n",
    "import random\n",
    "import string\n",
    "# 生成随机中英文混合文本的辅助函数（无需额外依赖）\n",
    "def gen_mixed_text(min_len=20, max_len=80):\n",
    "    chinese_chars = '你好世界天气人工智能自然语言处理测试示例随机文本中文字符'\n",
    "    result = []\n",
    "    length = random.randint(min_len, max_len)\n",
    "    for _ in range(length):\n",
    "        if random.random() < 0.5:\n",
    "            # 选择一个中文字符或短词（简单处理）\n",
    "            result.append(random.choice(chinese_chars))\n",
    "        else:\n",
    "            # 生成一个英文单词或字符片段\n",
    "            word_len = random.randint(1, 8)\n",
    "            word = ''.join(random.choice(string.ascii_lowercase) for _ in range(word_len))\n",
    "            result.append(word)\n",
    "    # 用空格分隔英文片段，中文字符直接相连，返回字符串\n",
    "    return ' '.join([r for r in result if all(ord(c) < 128 for c in r)]) + ''.join([r for r in result if not all(ord(c) < 128 for c in r)])\n",
    "\n",
    "# 生成示例文本并赋值给 s\n",
    "s = gen_mixed_text()\n",
    "segments = re.split(r'(\\w+)', s)  # 简单将英文字母/数字分段（保留），中文段落留在其它项\n",
    "tokens = []\n",
    "for seg in segments:\n",
    "    if re.fullmatch(r'\\w+', seg):  # 英文/数字片段\n",
    "        tokens.extend(nltk.word_tokenize(seg))\n",
    "    else:  # 中文或其它字符\n",
    "        tokens.extend(jieba.lcut(seg))\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
