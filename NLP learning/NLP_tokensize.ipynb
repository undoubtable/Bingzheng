{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b092d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "import nltk\n",
    "\n",
    "text = \"Hello, world! This is a test sentence for tokenization.\"\n",
    "# 先确保所需的 punkt 资源已安装，避免在第一次调用时抛出 LookupError\n",
    "for resource in ('punkt', 'punkt_tab'):\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}')\n",
    "    except LookupError:\n",
    "        print(f'NLTK resource {resource} not found. Downloading...')\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "print(\"Word Tokenization:\")\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d009ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens = ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 现在进行分词，并捕获任何剩余的 LookupError 以便给出明确提示\n",
    "try:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    print('tokens =', tokens)\n",
    "except LookupError as e:\n",
    "    print('Failed to tokenize: missing NLTK resource. Please run nltk.download(\"punkt\") or nltk.download(\"punkt_tab\") manually.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff19bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\admin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.375 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens = ['在', '一个', '遥远', '的', '城市', '里', '，', '清晨', '的', '阳光', '穿过', '高楼', '之间', '的', '缝隙', '，', '洒', '在', '尚未', '醒来', '的', '街道', '上', '。']\n",
      "词数 = 24\n"
     ]
    }
   ],
   "source": [
    "# 中文分词示例（使用 jieba）\n",
    "import jieba\n",
    "test1 = \"在一个遥远的城市里，清晨的阳光穿过高楼之间的缝隙，洒在尚未醒来的街道上。\"\n",
    "# 假设 test1 已在 notebook 中定义\n",
    "# test1 = \"在一个遥远的城市里，清晨的阳光穿过高楼之间的缝隙，洒在尚未醒来的街道上。\"\n",
    "# 精确模式，返回 list\n",
    "tokens = jieba.lcut(test1)\n",
    "print('tokens =', tokens)\n",
    "print('词数 =', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fef1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "['fc', ' ', 'dkyiufku', ' ', 'yakdiqf', ' ', 'mspjdw', ' ', 'crqesn', ' ', 'toxnzc', ' ', 'veb', ' ', 'yllytes', ' ', 'ri', ' ', 'utxh', ' ', 'ikfgc', ' ', 'u', ' ', 'dopk', ' ', 'zmlt', ' ', 'pmvxfvf', ' ', 'cynuw', ' ', 'vaydf', ' ', 'ezi', ' ', 'd', ' ', 'dtmz', ' ', 'ckqg', ' ', 'ntgdjqd', ' ', 'yxhvedpm', ' ', 'yqwdbxmt', ' ', 'ht', ' ', 'jtz', ' ', 'bd', ' ', 'nxekzzty你中自语本能中例测机智天自气人文例天界字试例理文世天界语']\n"
     ]
    }
   ],
   "source": [
    "# 安装缺失的第三方包（在 notebook 中使用魔法命令）\n",
    "%pip install jieba --quiet\n",
    "\n",
    "import re\n",
    "import jieba\n",
    "import nltk\n",
    "\n",
    "import random\n",
    "import string\n",
    "# 生成随机中英文混合文本的辅助函数（无需额外依赖）\n",
    "def gen_mixed_text(min_len=20, max_len=80):\n",
    "    chinese_chars = '你好世界天气人工智能自然语言处理测试示例随机文本中文字符'\n",
    "    result = []\n",
    "    length = random.randint(min_len, max_len)\n",
    "    for _ in range(length):\n",
    "        if random.random() < 0.5:\n",
    "            # 选择一个中文字符或短词（简单处理）\n",
    "            result.append(random.choice(chinese_chars))\n",
    "        else:\n",
    "            # 生成一个英文单词或字符片段\n",
    "            word_len = random.randint(1, 8)\n",
    "            word = ''.join(random.choice(string.ascii_lowercase) for _ in range(word_len))\n",
    "            result.append(word)\n",
    "    # 用空格分隔英文片段，中文字符直接相连，返回字符串\n",
    "    return ' '.join([r for r in result if all(ord(c) < 128 for c in r)]) + ''.join([r for r in result if not all(ord(c) < 128 for c in r)])\n",
    "\n",
    "# 生成示例文本并赋值给 s\n",
    "s = gen_mixed_text()\n",
    "segments = re.split(r'(\\w+)', s)  # 简单将英文字母/数字分段（保留），中文段落留在其它项\n",
    "tokens = []\n",
    "for seg in segments:\n",
    "    if re.fullmatch(r'\\w+', seg):  # 英文/数字片段\n",
    "        tokens.extend(nltk.word_tokenize(seg))\n",
    "    else:  # 中文或其它字符\n",
    "        tokens.extend(jieba.lcut(seg))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64db2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "example_text = \"This is an example sentence for tokenization.\"\n",
    "tokens = word_tokenize(example_text)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
